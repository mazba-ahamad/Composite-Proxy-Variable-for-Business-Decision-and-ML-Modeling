#!/usr/bin/env python
# coding: utf-8

# In[167]:


#Store Selection Model using MCDA Framework


# In[76]:


#Importing required packages
import pandas as pd
from scikitmcda.topsis import TOPSIS
#pip install TOPSIS-Manmeet-101803095
#source: https://pypi.org/project/TOPSIS-Manmeet-101803095/
from topsis import TopsisScore


# In[88]:


#Importing store data
df = pd.read_csv('T:\Job Applications-Industry\CVS_Mazbahul Ahamad\df_new.csv') 
df = df.loc[ : , df.columns != 'id']
df


# In[89]:


# Assigning weights (w) and impacts(im)
w = [.10,.10,.10,.10,.10,.10,.10,.10,.10,.10]
im = ['+','+','+','+','-','-','+','+','-','+']


# In[90]:


# No fo parameter required for TOPSIS
noofparameterreq = len(df.columns)


# In[91]:


if (len(w) != noofparameterreq) or (len(im) != noofparameterreq) :
        print("The no of parameters required for weights and impacts is : " +                   
        str(noofparameterreq))
        raise Exception('Inavalid no of parameters in weights or impacts')


# In[92]:


df1 = TopsisScore(df,w,im)
df1.head()


# In[93]:


df1.sort_values(['Rank'], ascending=True)


# In[94]:


df1['Topsis Score'].describe()


# In[95]:


binned = pd.cut(df1["Topsis Score"], bins = [.23,.56,.77], labels = [0,1])
df1["proxy_tar"] = binned
df1


# In[96]:


df1['proxy_tar'].describe()


# In[97]:


df2 = df1.drop(["Topsis Score", "Rank"],axis=1)
df2


# In[98]:


#save result
#df1.to_csv('T:Job Applications-Industry\CVS_Mazbahul Ahamad\df-nan.csv')


# In[168]:


#Filtering Warnings
import warnings
warnings.filterwarnings('ignore')


# In[169]:


#Importing required packages
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier


# In[170]:


#Dividing Data in test and train
X_train,X_test,y_train,y_test=train_test_split(df2.iloc[:,[0,1,2,3,4,5,6,7,8,9]],df2.iloc[:,[10]],test_size=0.25,random_state=429)


# In[171]:


#Logistic Regression Pipeline
LRPL=Pipeline([('myscaler',MinMaxScaler()),
                     ('mypca',PCA(n_components=4)),
                     ('logistic_classifier',LogisticRegression())])


# In[172]:


#Decision tree Pipeline
DTPL=Pipeline([('myscaler',MinMaxScaler()),
                     ('mypca',PCA(n_components=4)),
                     ('decisiontree_classifier',DecisionTreeClassifier())])


# In[173]:


#Random Forest Pipeline
RFPL=Pipeline([('myscaler',MinMaxScaler()),
                     ('mypca',PCA(n_components=4)),
                     ('randomforest_classifier',RandomForestClassifier())])


# In[174]:


## Defining the pipelines in a list
mypipeline = [LRPL, DTPL, RFPL]


# In[175]:


#Defining variables for choosing best model
accuracy=0.0
classifier=0
pipeline=""


# In[176]:


# Creating dictionary of pipelines and training models
PipelineDict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'Random Forest'}

# Fit the pipelines
for mypipe in mypipeline:
    mypipe.fit(X_train, y_train)


# In[177]:


#getting test accuracy for all classifiers
for i,model in enumerate(mypipeline):
    print("{} Test Accuracy: {}".format(PipelineDict[i],model.score(X_test,y_test)))


# In[186]:


#Choosing best model for the given data
for i,model in enumerate(mypipeline):
    if model.score(X_test,y_test)>accuracy:
        accuracy=model.score(X_test,y_test)
        pipeline=model
        classifier=i
print('Classifier with best accuracy:{}'.format(PipelineDict[classifier]))


# In[ ]:


# Logistic Regression


# In[198]:


#Dividing Data in test and train
df3=df2
X_train1,X_test1,y_train1,y_test1=train_test_split(df3.iloc[:,[0,1,2,3,4,5,6,7,8,9]],df3.iloc[:,[10]],test_size=0.25,random_state=429)


# In[215]:


import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

LRModel = LogisticRegression()
LRModel.fit(X_train1,y_train1)


# In[216]:


importances = pd.DataFrame(data={
    'Attribute': X_train1.columns,
    'Importance': LRModel.coef_[0]
})
importances = importances.sort_values(by='Importance', ascending=False)
print(importances)


# In[217]:


from matplotlib import pyplot as plt

plt.bar(x=importances['Attribute'], height=importances['Importance'])
#plt.bar(x=importances['Importance'], height=importances['Attribute'])
plt.title('Feature importances obtained from coefficients', size=20)
plt.xticks(rotation='vertical')
plt.show()


# In[157]:


y_pred = LR.predict(X_test)
y_pred


# In[165]:


from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
from sklearn.metrics import accuracy_score 
print ("Accuracy: ", accuracy_score(y_test, y_pred))


# In[152]:


#yhat_prob = LR.predict_proba(X_test)
#yhat_prob


# In[153]:


from sklearn.metrics import jaccard_score
jaccard_score(y_test, yhat,pos_label=0)


# In[166]:





# In[179]:


importances = pd.DataFrame(data={
    'Attribute': X_train.columns,
    'Importance': model.coef_[0]
})
importances = importances.sort_values(by='Importance', ascending=False)


# In[ ]:




